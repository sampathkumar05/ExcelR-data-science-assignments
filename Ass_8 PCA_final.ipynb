# -*- coding: utf-8 -*-
"""
Created on Sat Dec  2 17:25:01 2023

@author: SAMPATH
"""

#PRINCIPLE COMPONENT ANALYSIS

# completed
#Importing the data
import pandas as pd
import numpy as np
ass8=pd.read_csv("wine.csv")
ass8
ass8.dtypes
list(ass8)
ass8.shape
ass8.head()


#iloc
X = ass8.iloc[:,0:]
#[allcolumns,starting index:ending index]


# standardization
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_X = SS.fit_transform(X)
SS_X = pd.DataFrame(SS_X)
SS_X


#PCA
from sklearn.decomposition import PCA
pca=PCA()

PC=pca.fit_transform(SS_X)

PC_df=pd.DataFrame(PC)
PC_df.head()

PC_df.iloc[:,0].var()
PC_df.iloc[:,1].var()
PC_df.iloc[:,2].var()

pca.explained_variance_ratio_
pca_ev = pd.DataFrame(pca.explained_variance_ratio_)
pca_ev*100


PC_X = PC_df.iloc[:,0:3]
PC_X


#a)AGGLOMERATIVE CLUSTER==============================
#forming a group using  clusters

from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='complete')
Y = cluster.fit_predict(PC_X)
PC_X["Y"] = pd.DataFrame(Y)
PC_X["Y"].value_counts()
PC_X

#B)K.MEANS CLUSTER=================================
from sklearn.cluster import KMeans
KMeans = KMeans(n_clusters=5,n_init=30)
KMeans.fit(PC_X)
PC_X["Kmeans_Y"]=pd.DataFrame(Y)
PC_X["Kmeans_Y"].value_counts()

KMeans.inertia_

#To identify the best K value from all possible K values
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
inertia=[]
for i in range(1,11):
    km=KMeans(n_clusters=i,random_state=0)
    km.fit(PC_X)
    inertia.append(km.inertia_)
    
print(inertia)
plt.plot(range(1,11),inertia)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('inertia')
plt.show()


#COMPARISION

k_optimal = 3  # Change this to the number you observe from the Elbow plot

# Perform K-Means clustering with the optimal number of clusters
kmeans_optimal = KMeans(n_clusters=k_optimal, init='k-means++', random_state=42)
ass8['cluster_kmeans'] = kmeans_optimal.fit_predict(PC_X)

# Compare the obtained clusters with the original 'class' column
cluster_comparison = ass8.crosstab(ass8['class'], ass8['cluster_kmeans'])
print(cluster_comparison)



#======================================================================
#CHATGPT

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'class' is the column you want to ignore
X = df.drop('class', axis=1)

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA
pca = PCA(n_components=3)
principal_components = pca.fit_transform(X_scaled)
principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])

# K-Means clustering with scree plot
wcss = []  # Within-cluster sum of squares

for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(principal_df)
    wcss.append(kmeans.inertia_)

# Scree plot or elbow curve
plt.plot(range(1, 11), wcss)
plt.title('Scree Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')  # Within-cluster sum of squares
plt.show()

# Hierarchical clustering
linked = linkage(principal_df, 'ward')
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.show()

# Based on the plots, determine the optimal number of clusters for K-Means

# Assuming you found that the optimal number of clusters is 'k_optimal'
k_optimal = 3

# Perform K-Means clustering with the optimal number of clusters
kmeans_optimal = KMeans(n_clusters=k_optimal, init='k-means++', max_iter=300, n_init=10, random_state=0)
clusters = kmeans_optimal.fit_predict(principal_df)

# Compare the clustering results with the original 'class' column
comparison_df = pd.DataFrame({'Original': df['class'], 'Cluster': clusters})
print(comparison_df)


#==+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#FINAL CODE

#principle component analysis- when we have n no.of variables apply the PCA to reduce the variables 
#IMPORTING THE DATA
import pandas as pd
import numpy as np
df=pd.read_csv("wine.csv")
df
df.dtypes
list(df)
df.shape
df.head()

#SPLITTING
X = df.iloc[:,0:]
#[allcolumns,starting index:ending index]


#standardization
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_X = SS.fit_transform(X)
SS_X = pd.DataFrame(SS_X)
SS_X

#PCA
from sklearn.decomposition import PCA
pca=PCA()

PC=pca.fit_transform(SS_X)

PC_df=pd.DataFrame(PC)
PC_df.head()

PC_X = PC_df.iloc[:,0:3]
PC_X.columns=['PC1', 'PC2', 'PC3']
PC_X

#Applying clustering on SS_X and PC_X to create a target variable and compare

#K.MEANS CLUSTER on SS_X
from sklearn.cluster import KMeans
KMeans = KMeans(n_clusters=3,n_init=30)
KMeans.fit(SS_X)
Y=KMeans.fit_predict(SS_X)
df["Kmeans_SS_Y"]=pd.DataFrame(Y)
df["Kmeans_SS_Y"].value_counts()

KMeans.inertia_

df

#To identify the best K value from all possible K values
from sklearn.cluster import KMeans
inertia=[]
for i in range(1,11):
    km=KMeans(n_clusters=i,random_state=0)
    km.fit(SS_X)
    inertia.append(km.inertia_)
    
print(inertia)
plt.plot(range(1,11),inertia,linestyle='--',marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('inertia')
plt.show()

#Hierarchical clustering on SS_X
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
linked = linkage(SS_X, 'ward')
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram on SS_X')
plt.show()


#K.MEANS CLUSTER on PC_X
from sklearn.cluster import KMeans
KMeans = KMeans(n_clusters=3,n_init=30)
KMeans.fit(PC_X)
Y=KMeans.fit_predict(PC_X)
PC_X["Kmeans_PC_Y"]=pd.DataFrame(Y)
PC_X["Kmeans_PC_Y"].value_counts()

KMeans.inertia_

PC_X

#To identify the best K value from all possible K values
from sklearn.cluster import KMeans
inertia=[]
for i in range(1,11):
    km=KMeans(n_clusters=i,random_state=0)
    km.fit(PC_X)
    inertia.append(km.inertia_)
    
print(inertia)
plt.plot(range(1,11),inertia,linestyle='--',marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('inertia')
plt.show()

#Hierarchical clustering on PC_X

from scipy.cluster.hierarchy import dendrogram, linkage
linked = linkage(PC_X, 'ward')
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram on PC_X')
plt.show()

#comparision
comparison_df = pd.DataFrame({'Original': df['Kmeans_SS_Y'], 'PCA': PC_X['Kmeans_PC_Y']})
print(comparison_df)


#CONCLUSION
'''
WE RECIEVED THE SAME NUMBER OF CLUSTERS I.E 3 FOR BOTH ORIGINAL DATA AND PRINCIPLE COMPONENT ANALYSIS DATA BY OBSERVING THE ELBOW PLOT AND DENDOGRAM. 
'''