# -*- coding: utf-8 -*-
"""
Created on Mon Jan  8 16:22:05 2024

@author: SAMPATH
"""
##############################################ASSIGNMENT_11 TEXT_MINING################################################################################

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# importing the data
tweets = pd.read_csv('Elon_musk.csv',encoding='cp1252')
tweets


tweets.drop(['Unnamed: 0'], inplace= True, axis= 1)
tweets


# Data Exploration
import itertools
c = list(itertools.chain(*tweets.Text.map(lambda t: [handle.replace(":", " ")[1:] for handle in t.split(" ")
                                                     if '@' in handle.replace(":", " ")]).tolist()))
pd.Series(c).value_counts().head(20).plot.bar(figsize=(14, 7), fontsize=16, color='lightcoral')
plt.gca().set_title('@elonmusk top user tags', fontsize=20)
plt.gca().set_xticklabels(plt.gca().get_xticklabels(), rotation=90, ha='right', fontsize=16)
pass


# BasicText Preprocessing
# Cleaning the text by removing irrelevant information

import re #regular expression
import string
def clean_text(text):
    text = text.lower()
    text = re.sub('\\[.*?\\]', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\\w*\\d\\w*', '', text)
    text = re.sub("[0-9" "]+"," ",text)
    text = re.sub('[‘’“”…]', '', text)
    return text

clean = lambda x: clean_text(x)
tweets["Text"] = tweets["Text"].apply(clean_text) # Clean The Text
tweets.head(10)

# Word Frequency
freq = pd.Series(' '.join(tweets['Text']).split()).value_counts()[:20] # for top 20
freq

# Removing the stopwords
from nltk.corpus import stopwords
stop = stopwords.words('english')
tweets['Text'] = tweets['Text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

# Word Frequency after Removal of Stopwords
freq_stp = pd.Series(' '.join(tweets['Text']).split()).value_counts()[:20] # for top 20
freq_stp

# Basic Feature Extaction
# Count vectoriser tells the frequency of a word.
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
vectorizer = CountVectorizer(min_df = 1, max_df = 0.9)
X = vectorizer.fit_transform(tweets["Text"])
word_freq_df = pd.DataFrame({'term': vectorizer.get_feature_names_out(), 'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})
word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])
word_freq_df.head(11)


# TFIDF - Term frequency inverse Document Frequency
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) #keep top 1000 words
doc_vec = vectorizer.fit_transform(tweets["Text"])
names_features = vectorizer.get_feature_names_out()
dense = doc_vec.todense()
denselist = dense.tolist()

df = pd.DataFrame(denselist, columns = names_features)
df

# N-gram
# Bi-gram
def get_top_n2_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(2,2), max_features=2000).fit(corpus) #for tri-gram, put ngram_range=(3,3)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]
    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
top2_words = get_top_n2_words(tweets["Text"], n=200) #top 200
top2_df = pd.DataFrame(top2_words)
top2_df.columns = ["Bi-gram", "Freq"]
top2_df.head(11)

import matplotlib.pyplot as plt
import seaborn as sns
top20_bigram = top2_df.iloc[0:20,:]
fig = plt.figure(figsize = (10, 5))
plot = sns.barplot(x=top20_bigram["Bi-gram"],y=top20_bigram["Freq"])
plot.set_xticklabels(rotation=90,labels = top20_bigram["Bi-gram"])
                                                       

#Tri-gram
def get_top_n3_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(3,3), max_features=2000).fit(corpus)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
top3_words = get_top_n3_words(tweets["Text"], n=200)
top3_df = pd.DataFrame(top3_words)
top3_df.columns = ["Tri-gram", "Freq"]
top3_df.head(11)                      


import seaborn as sns
top20_trigram = top3_df.iloc[0:20,:]
fig = plt.figure(figsize = (10, 5))
plot = sns.barplot(x=top20_trigram["Tri-gram"],y=top20_trigram["Freq"])
plot.set_xticklabels(rotation=90,labels = top20_trigram["Tri-gram"])    

# Wordcloud
string_Total = " ".join(tweets["Text"])

#wordcloud for entire corpus
from wordcloud import WordCloud
plt.figure(figsize=(12,12),dpi=90)
wordcloud_stw = WordCloud(background_color= 'black', width = 1800, height = 1500).generate(string_Total)
plt.imshow(wordcloud_stw)

# Subjectivity and Polarity
from textblob import TextBlob
# Get The Subjectivity
def sentiment_analysis(ds):
    sentiment = TextBlob(ds["Text"]).sentiment
    return pd.Series([sentiment.subjectivity, sentiment.polarity])
# Adding Subjectivity & Polarity
tweets[["subjectivity", "polarity"]] = tweets.apply(sentiment_analysis, axis=1)
tweets.head(11)


import matplotlib.pyplot as plt
from wordcloud import WordCloud
allwords = " ".join([twts for twts in tweets["Text"]])
wordCloud = WordCloud(width = 1800, height = 1500, random_state = 21).generate(allwords)
plt.figure(figsize=(12,12), dpi=90)
plt.imshow(wordCloud, interpolation = "bilinear")
plt.axis("off"),
plt.show()

# 1 way:
def fetch_sentiment_using_SIA(text):
    sid = SentimentIntensityAnalyzer()
    polarity_scores = sid.polarity_scores(text)
    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'

# 2 way:
def fetch_sentiment_using_textblob(text):
    analysis = TextBlob(text)
    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'

# Compute The Negative, Neutral, Positive Analysis
def analysis(score):
    if score < 0:
        return "Negative"
    elif score == 0:
        return "Neutral"
    else:
        return "Positive"
    
# Create a New Analysis Column
tweets["analysis"] = tweets["polarity"].apply(analysis)
positive_tweets = tweets[tweets['analysis'] == 'Positive']
negative_tweets = tweets[tweets['analysis'] == 'Negative']

print('positive tweets')
for i, row in positive_tweets[:5].iterrows():
    print('-' + row['Text'])

print('negative tweets')
for i, row in negative_tweets[:5].iterrows():
    print('-' + row['Text'])
  
# Showing The Graph  
plt.figure(figsize=(10, 8))
for i in range(0, tweets.shape[0]):
    plt.scatter(tweets["polarity"][i], tweets["subjectivity"][i], color = "Red")

plt.title("Sentiment Analysis") 
plt.xlabel("Polarity")
plt.ylabel("Subjectivity") 
plt.show() 

print((len(positive_tweets) / len(negative_tweets))*100)


# Conclusion: Since that number is positive, and quite high of a ratio, we can also conclude that Elon Musk is a positive guy

#1b)

import requests
import re
from bs4 import BeautifulSoup
import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline


headers = {'authority': 'www.amazon.in',
            'accept-language': 'en-US,en;q=0.9',
            'referer': 'https://www.amazon.in/boAt-Immortal-141-Signature-Resistance/dp/B0C7QWGZ6Z/?_encoding=UTF8&pd_rd_w=94bIX&content-id=amzn1.sym.87579d35-fd9c-4df4-a4dd-ff69ee6fde5e%3Aamzn1.symc.acc592a4-4352-4855-9385-357337847763&pf_rd_p=87579d35-fd9c-4df4-a4dd-ff69ee6fde5e&pf_rd_r=DY337VK4ZWXR8VRGGK6X&pd_rd_wg=Y1TtY&pd_rd_r=9ab9a7f5-6031-4296-87ee-68e97cc72503&ref_=pd_gw_ci_mcx_mr_hp_d',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}

# Looping through multiple pages

def get_soup(url):
    r = requests.get(url, headers=headers,
    params={'url': url, 'wait': 2}),
    soup = BeautifulSoup(r.text, 'html.parser'),
    return soup


reviewlist = []

def get_reviews(soup):
    reviews = soup.find_all('div', {'data-hook': 'review'})
    try:
        for item in reviews:
            review = {'Title': item.find('a', {'data-hook': 'review-title'}).text.strip(),
                      'Rating':float(item.find('i', {'data-hook': 'review-star-rating'}).text.replace('out of 5 stars', '').strip()),
                      'Review':item.find('span', {'data-hook': 'review-body'}).text.strip(),
                      'Review_Date':item.find('span', {'data-hook': 'review-date'}).text.replace('Reviewed in India 🇮🇳 on', 'Reviewed in India on').strip()},
            reviewlist.append(review)
    except:
        pass
 
# Import tqdm for console-based progress bars
from tqdm import tqdm  

for x in tqdm(range(160)):  # Use tqdm instead of tqdm_notebook,
    soup = get_soup('https://www.amazon.in/boAt-Immortal-141-Signature-Resistance/product-reviews/B0C7QWGZ6Z/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews')
    get_reviews(soup)

    if not soup.find('li', {'class': 'a-disabled a-last'}):
        pass
    else:
        break
    
# Save results to a Dataframe, then export as CSV

df = pd.DataFrame(reviewlist)
df

df["Review_Date"] = df["Review_Date"].replace('Reviewed in India on','', regex=True)
df.head(10)

df.to_csv("boAt-Immortal-141_Review.csv")


#load the data set

reviews = pd.read_csv('BoatSmartWatch_Review.csv')
reviews.drop(['Unnamed: 0'],inplace=True,axis=1)
reviews
   

#EDA - EXPLORATORY DATA ANALYSIS
reviews.Rating.describe()

reviews.info()

reviews.isna().sum()

reviews.dropna(subset=['Review'],inplace=True)
reviews.head()

#DATA VISUALIZATION

#Creating a dataframe
dayreview = pd.DataFrame(reviews.groupby('Review_Date')['Review'].count()).reset_index()
dayreview['Date'] = dayreview['Review_Date']
dayreview.sort_values(by = ['Review_Date'])

#Plotting the graph
plt.figure(figsize=(16,6)),
sns.barplot(x = "Date", y = "Review", data = dayreview)
plt.title('Date vs Reviews Count', fontsize=16)
plt.xticks(rotation=90,fontsize=7)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Reviews Count',fontsize=14)
plt.show()


#percentage of ratings

plt.figure(figsize=(10,8))
plt.title('Percentage of Ratings')
ax = sns.countplot(y = 'Rating', data = reviews)
total = len(reviews)
for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_width()/total)
        x = p.get_x() + p.get_width() + 0.02
        y = p.get_y() + p.get_height()/2
        ax.annotate(percentage, (x, y))
        

# BasicText Preprocessing for Sentiment Analysis
# Spelling Correction
reviews['Review'][:5].apply(lambda x: str(TextBlob(x).correct()))


boat_review = reviews[['Review']]
boat_review

# RESETTING INDEX
boat_review.reset_index(drop=True, inplace=True)
stop_words = stopwords.words('english')
boat_review['Review'] = boat_review['Review'].apply(lambda x:.join(x.lower() for x in str(x).split() if x not in stop_words))


#LEMMITIZER

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Removing punctuation, making str to lower, applying Lemmatizer, Removing Stop words
corpus=[]
for i in tqdm(range(0, len(boat_review))):
    cleaned = re.sub('[^a-zA-Z]', boat_review["Review"][i])
    cleaned = cleaned.lower()
    cleaned = cleaned.split()
    cleaned = [lemmatizer.lemmatize(word) for word in cleaned if word not in stopwords.words("english")]
    cleaned = ' '.join(cleaned)
    corpus.append(cleaned)


# Saving cleaned data to compare with original data, to ckeck amount of information lost
dataframe = pd.DataFrame({"Clean_Review": corpus, "Uncleaned_Reviews": boat_review.Review})                      
dataframe.head()

# Text Pre-processing Techniques
#Removing '@names'
def remove_pattern(text, pattern_regex):
    r = re.findall(pattern_regex, text
                   for i in r:
        text = re.sub(i, '', text)
    return text 

# We are keeping cleaned tweets in a new column called 'tidy_tweets'
dataframe['Clean_Reviews'] = np.vectorize(remove_pattern)(dataframe['Clean_Reviews'], "@[\\w]*")
dataframe.head(10)

# Removing links (http | https)
cleaned_reviews = []
for index, row in dataframe.iterrows():
    words_without_links = [word for word in row.Clean_Reviews.split() if 'http' not in word]
    cleaned_reviews.append(' '.join(words_without_links))
    
dataframe['Clean_Reviews'] = cleaned_reviews
dataframe.head(10)


#Removing Review with empty text
dataframe = dataframe[dataframe['Clean_Reviews']!='']
dataframe.head(10)

#Dropping duplicate rows
dataframe.drop_duplicates(subset=['Clean_Reviews'], keep=False)
dataframe.head(10)

#Function to remove emoji
def remove_emoji(text):
    emoji_pattern = re.compile([
                          "U0001F600-U0001F64F",   # emotions
                          "U0001F300-U0001F5FF",   # symbols & pictographs
                          "U0001F680-U0001F6FF",   # transport & map symbols
                          "U0001F1E0-U0001F1FF",   # flags (iOS)
                          "U00002702-U000027B0",
                          "U000024C2-U0001F251",
                           ], flags=re.UNICODE),
    return emoji_pattern.sub(r'', text)

dataframe['Clean_Reviews']=dataframe['Clean_Reviews'].apply(lambda x: remove_emoji(x))
dataframe.head(10)

#Removing Stop words
import codecs
with codecs.open("stop.txt", "r", encoding="ISO-8859-1") as s:
    stop = s.read()
    print(stop[:101])
    
    
stop.split(" ")  
    

my_stop_words = stopwords.words('english')
sw_list = [stop]
my_stop_words.extend(sw_list)
stopwords_set = set(my_stop_words)
cleaned_tweets = []
for index, row in dataframe.iterrows():
# filerting out all the stopwords 
    words_without_stopwords = [word for word in row.Clean_Reviews.split() if not word in stopwords_set and '#' not in word.lower()]
# finally creating tweets list of tuples containing stopwords(list) and sentimentType 
    cleaned_tweets.append(' '.join(words_without_stopwords))
    
dataframe['Clean_Reviews'] = cleaned_tweets
dataframe.head(10)
   

#Tokenize 'Clean_Reviews'
TextBlob(dataframe['Clean_Reviews'][1]).words
tokenized_review = dataframe['Clean_Reviews'].apply(lambda x: x.split())
tokenized_review.head(10)


dataframe['Tokenized_Reviews'] = dataframe['Clean_Reviews'].apply(lambda x: nltk.word_tokenize(x))
dataframe.head(10)
   

# Converting words to Stemmer
from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("english")
xx = pd.DataFrame()
xx['stemmed'] = dataframe['Clean_Reviews'].apply(lambda x: " ".join([stemmer.stem(word) for word in x.split()]))
xx

# Converting words to Lemma
from nltk.stem import WordNetLemmatizer
word_lemmatizer = WordNetLemmatizer()
yy = pd.DataFrame()
yy['stemmed'] = dataframe['Clean_Reviews'].apply(lambda x: " ".join([word_lemmatizer.lemmatize(i) for i in x.split()]))
yy
   

# Basic Feature Extaction     
# Applying bag of Words without N grams

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer()
tweetscv=cv.fit_transform(dataframe.Clean_Reviews)
print(cv.get_feature_names_out()[109:200])
 
print(cv.get_feature_names_out()[:100])

print(tweetscv.toarray()[100:200])

#CountVectorizer with N-grams (Bigrams & Trigrams)
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import PorterStemmer
dataframe = dataframe.reset_index(drop=True)
ps = PorterStemmer()
corpus = []
for i in tqdm(range(0, len(dataframe))):
    review = re.sub('[^a-zA-Z]', ' ', dataframe['Clean_Reviews'][i])
    review = review.lower()
    review = review.split()
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)


corpus[3]

#Applying Countvectorizer (Creating the Bag of Words model)
cv = CountVectorizer(max_features=1546,ngram_range=(1,3))
X = cv.fit_transform(corpus).toarray()
X.shape


cv.get_feature_names_out()[:20]

cv.get_params()

count_df = pd.DataFrame(X, columns=cv.get_feature_names_out())
count_df


#TF-IDF Vectorizer
ps = PorterStemmer()
corpus = []
for i in tqdm(range(0, len(dataframe))):
    review = re.sub('[^a-zA-Z]', ' ', dataframe['Clean_Reviews'][i])
    review = review.lower()
    review = review.split()
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

corpus[4]

# Applying TF-IDF Vectorizer
tfidf_v = TfidfVectorizer(max_features=1546,ngram_range=(1,3))
X = tfidf_v.fit_transform(corpus).toarray()
X.shape

tfidf_v.get_feature_names_out()[:20]

tfidf_v.get_params()

count_df = pd.DataFrame(X, columns=tfidf_v.get_feature_names_out())
count_df

# Named Entity Recognition (NER)
reviews = [review.strip() for review in dataframe.Clean_Reviews]
reviews = [comment for comment in reviews if comment]

# Joining the list into one string/text
reviews_text = ' '.join(reviews)
reviews_text[0:2000]


#Feature Extraction

#1. BOW Features
bow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')

# 2.bag-of-words feature matrix
bow_word_feature = bow_word_vectorizer.fit_transform(dataframe['Clean_Reviews'])

# 3.TF-IDF Features
tfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english')

# 4.TF-IDF feature matrix
tfidf_word_feature = tfidf_word_vectorizer.fit_transform(dataframe['Clean_Reviews'])

# 5.Fetch sentiments Using TextBlob
def fetch_sentiment_using_textblob(text):
    analysis = TextBlob(text)
    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'

sentiments_using_textblob = dataframe.Clean_Reviews.apply(lambda reviews_text: fetch_sentiment_using_textblob(reviews_text))
pd.DataFrame(sentiments_using_textblob.value_counts())
   
# calculate subjectivity and Polarity
def calc_subj(text):
    return TextBlob(text).sentiment.subjectivity

# function for Polarity
def calc_pola(text):
    return TextBlob(text).sentiment.polarity

dataframe['Subjectivity'] = dataframe.Clean_Reviews.apply(calc_subj)
dataframe['Polarity'] = dataframe.Clean_Reviews.apply(calc_pola)
dataframe.head()


axes = plt.subplots(figsize = (14,10))
plt.scatter(dataframe.Polarity, dataframe.Subjectivity)
plt.title('Sentiment Analysis')
plt.xlabel('Polarity')
plt.ylabel('Subjectivity')


#Story Generation and Visualization
allWords_ = ' '.join([review for review in dataframe[:500]['Clean_Reviews']])
axes = plt.subplots(figsize=(14,10))
wordcloud= WordCloud(background_color = 'black', width = 1800, height =1400).generate(allWords_)
plt.imshow(wordcloud)



# Most common words in positive Review
def generate_wordcloud(all_words):
    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)
    plt.figure(figsize=(14,10))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.show()
all_words = ' '.join([text for text in dataframe['Clean_Reviews'][dataframe.sentiment == 'pos']])
generate_wordcloud(all_words)


# Most common words in negative Review 
all_words = ' '.join([text for text in dataframe['Clean_Reviews'][dataframe.sentiment == 'neg']])
generate_wordcloud(all_words)
 
 
 
 
 
#SAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATH

 
 
 
 