# -*- coding: utf-8 -*-
"""
Created on Sat Dec 23 15:22:25 2023

@author: SAMPATH
"""
###############################ASSOCIATION RULES########################################

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt

###1) book data set##############################################################

# Load your boolean dataset
df = pd.read_csv("book.csv")
df

# Convert boolean values to column names
def boolean_to_column_names(row):
    return [col for col, value in row.items() if value == 1]   


# Apply the function to each row
transactions = df.apply(boolean_to_column_names, axis=1).tolist()
transactions


#Tring different values of support, confidence and minimun length
min_support_values = [0.001, 0.002, 0.003]
min_confidence_values = [0.2, 0.3, 0.4]
min_length_values = [2, 3, 4]
from apyori import apriori

for min_support in min_support_values:
    for min_confidence in min_confidence_values:
        for min_length in min_length_values:
           rules = apriori(transactions = transactions, min_support = min_support, min_confidence = min_confidence, min_lift = 3, 
                           min_length = min_length, 
                           max_length = 2)
           results=list(rules)          
           #EXTRACTING THE DATA IN LIST FORM
           a = [] 
           b = []
           c = []
           d = []
           e = []

           for i in range(0,len(results)):
              a.append(results[i][1]) # support
              b.append(results[i][2][0][0]) #base item
              c.append(results[i][2][0][1]) # add item
              d.append(results[i][2][0][2]) # confidence
              e.append(results[i][2][0][3]) # lift


           df_new = pd.concat([pd.DataFrame(a),pd.DataFrame(b),pd.DataFrame(c),pd.DataFrame(d),pd.DataFrame(e)],axis=1)
                     
           df_new.columns = ['Support','Baseitem','AddItem','Confidence','Lift']
           print("\n\n Support=" + str(min_support) + " , Confidence=" + str(min_confidence)+ " and min_length=" + str(min_length))
           print(df_new)
           
           # Scatter plot of support vs confidence
           plt.scatter(df_new['Support'], df_new['Confidence'], alpha=0.5)
           plt.xlabel('Support')
           plt.ylabel('Confidence')
           plt.title("Support vs. Confidence Scatter Plot when support=" + str(min_support) + " , confidence=" + str(min_confidence)+ " and min_length=" + str(min_length))
           plt.show()

           # Histogram of lift
           plt.hist(df_new['Lift'])
           plt.xlabel('Lift')
           plt.ylabel('Frequency')
           plt.title("Histogram of Lift support=" + str(min_support) + " , confidence=" + str(min_confidence)+ " and min_length=" + str(min_length))
           plt.show() 
           
           
           
  
#SAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATH
          
          
          
          
###2) my_movies data set##########################################################
    

# Load your boolean dataset
df = pd.read_csv("my_movies.csv")
df

# Convert boolean values to column names
def boolean_to_column_names(row):
    return [col for col, value in row.items() if value == 1]   


# Apply the function to each row
transactions = df.apply(boolean_to_column_names, axis=1).tolist()
transactions


#Tring different values of support, confidence and minimun length
min_support_values = [0.001, 0.002, 0.003]
min_confidence_values = [0.2, 0.3, 0.4]
min_length_values = [2, 3, 4]
from apyori import apriori

for min_support in min_support_values:
    for min_confidence in min_confidence_values:
        for min_length in min_length_values:
           rules = apriori(transactions = transactions, min_support = min_support, min_confidence = min_confidence, min_lift = 3, 
                           min_length = min_length, 
                           max_length = 2)
           results=list(rules)          
           #EXTRACTING THE DATA IN LIST FORM
           a = [] 
           b = []
           c = []
           d = []
           e = []

           for i in range(0,len(results)):
              a.append(results[i][1]) # support
              b.append(results[i][2][0][0]) #base item
              c.append(results[i][2][0][1]) # add item
              d.append(results[i][2][0][2]) # confidence
              e.append(results[i][2][0][3]) # lift


           df_new = pd.concat([pd.DataFrame(a),pd.DataFrame(b),pd.DataFrame(c),pd.DataFrame(d),pd.DataFrame(e)],axis=1)
                     
           df_new.columns = ['Support','Baseitem','AddItem','Confidence','Lift']
           print("\n\n Support=" + str(min_support) + " , Confidence=" + str(min_confidence)+ " and min_length=" + str(min_length))
           print(df_new)
           
           # Scatter plot of support vs confidence
           plt.scatter(df_new['Support'], df_new['Confidence'], alpha=0.5)
           plt.xlabel('Support')
           plt.ylabel('Confidence')
           plt.title("Support vs. Confidence Scatter Plot when support=" + str(min_support) + " , confidence=" + str(min_confidence)+ " and min_length=" + str(min_length))
           plt.show()

           # Histogram of lift
           plt.hist(df_new['Lift'])
           plt.xlabel('Lift')
           plt.ylabel('Frequency')
           plt.title("Histogram of Lift support=" + str(min_support) + " , confidence=" + str(min_confidence)+ " and min_length=" + str(min_length))
           plt.show() 
            
            
            
#SAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATHSAMPATH
            
            
            
###############################RECOMMENDATION RULES######################################## 


###book data set##########################################################


import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics.pairwise import linear_kernel

# Sample data (replace this with your actual dataset)
df=pd.read_csv("book.csv",encoding="latin1")
df

data = {
    'user_id': df['User_ID'],
    'book_title': df['Book_Title'],
    'book_rating': df['Book_Rating']
}

df = pd.DataFrame(data)

# Create a user-book matrix
user_book_matrix = df.pivot_table(index='user_id', columns='book_title', values='book_rating', fill_value=0)

# Normalize the ratings using Min-Max scaling
scaler = MinMaxScaler()
normalized_matrix = scaler.fit_transform(user_book_matrix)

# Calculate cosine similarity
cosine_similarities = cosine_similarity(normalized_matrix)

# Convert cosine similarities to DataFrame for better readability
cosine_sim_df = pd.DataFrame(cosine_similarities, index=user_book_matrix.index, columns=user_book_matrix.index)
cosine_sim_df

###
#Set the index and column names to user ids 
cosine_sim_df.index   = df.user_id.unique()
cosine_sim_df.columns = df.user_id.unique()

cosine_sim_df

#fill the diagonal elements with 0
np.fill_diagonal(cosine_similarities, 0)

#integer lock for first 7 rows and 7 columns
cosine_sim_df.iloc[0:7, 0:7]

#max value from the first 10 column values in a single columns
cosine_sim_df.idxmax(axis=1)[0:2182]


df[(df['user_id']==276726) | (df['user_id']==276744)]

#recommedations
for i in range(0,276726):    
        print("\n",i)
        if ([(df['user_id']==i)]):
            print( df[(df['user_id']==i) | (df['user_id']==i)])
    

###

